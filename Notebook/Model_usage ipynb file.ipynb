{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DataSet Analysis**"
      ],
      "metadata": {
        "id": "eeWrdD_ZeWpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "df = pd.read_csv('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
        "display(df.head())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4mYDlr0oe0l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Check the number of unique essay IDs**"
      ],
      "metadata": {
        "id": "ZULX8dYkeidI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_unique_essays = df['essay_id'].nunique()\n",
        "print(f\"Number of unique essays: {num_unique_essays}\")"
      ],
      "metadata": {
        "id": "a_Z2oDIteurU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Summary statistics of text length**"
      ],
      "metadata": {
        "id": "j4xWo02Gfb-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "df = pd.read_csv('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
        "\n",
        "df['text_length'] = df['full_text'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "def highlight_rows(row):\n",
        "    if row.name % 2 == 0:\n",
        "        return ['background-color: white'] * len(row)\n",
        "    else:\n",
        "        return ['background-color: lightblue'] * len(row)\n",
        "\n",
        "styled_df = df[['essay_id', 'text_length', 'score']].head(15).style.apply(highlight_rows, axis=1)\n",
        "display(styled_df)\n",
        "\n",
        "styled_stats = df['text_length'].describe().to_frame().style.set_table_styles([\n",
        "    {'selector': 'thead th', 'props': [('background-color', 'black'), ('color', 'white')]}]\n",
        ").applymap(lambda x: 'background-color:lightgreen')\n",
        "\n",
        "print(\"\\nText Length Statistics:\")\n",
        "display(styled_stats)\n"
      ],
      "metadata": {
        "id": "PXbSgFUzfh6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Plot a bar chart of score distribution**"
      ],
      "metadata": {
        "id": "kz2P_phngN6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=df['score'], palette='viridis')\n",
        "plt.xlabel(\"Essay Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Essay Scores\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IUkP2NZDgRwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Models Usage**\n",
        "\n",
        "**LinearRegression**\n",
        "\n",
        "\n",
        "**XGBRegreesor**\n",
        "\n",
        "**LSTM**\n",
        "\n",
        "**LGBM**\n",
        "\n",
        "**BERT**"
      ],
      "metadata": {
        "id": "lH2mKFIYg5OV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**M1. Linear Regression**"
      ],
      "metadata": {
        "id": "2Jy8N-99h2GN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbA7drmrqwNj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import joblib\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load your train.csv data\n",
        "df = pd.read_csv('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
        "\n",
        "# Download NLTK resource (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove non-alphanumeric characters and extra spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization using NLTK\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join tokens back into string\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing function to 'full_text' column\n",
        "df['clean_text'] = df['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "\n",
        "# Fit and transform the cleaned text\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
        "\n",
        "# Split data into training and validation sets (80-20 split)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(tfidf_matrix, df['score'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_pred = model.predict(X_valid)\n",
        "\n",
        "# Clip predictions to range [1, 5] as scores are typically in this range\n",
        "y_pred = np.clip(y_pred, 1, 5)\n",
        "\n",
        "# Calculate quadratic weighted kappa\n",
        "kappa = cohen_kappa_score(y_valid.round().astype(int), y_pred.round().astype(int), weights='quadratic')\n",
        "\n",
        "print(f\"Quadratic Weighted Kappa (Kappa): {kappa}\")\n",
        "\n",
        "# Save the model to Google Drive as HDF5 file\n",
        "model_filename = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/model.h5'\n",
        "joblib.dump(model, model_filename)\n",
        "print(f\"Model saved as {model_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTfLSvuGuUs7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Load the saved model and vectorizer\n",
        "model = joblib.load('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/essaymodel.h5')\n",
        "tfidf_vectorizer = joblib.load('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/tfidf_vectorizer.joblib')\n",
        "\n",
        "# Define function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing function to 'full_text' column\n",
        "df_test['clean_text'] = df_test['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Transform the cleaned text using the loaded TF-IDF vectorizer\n",
        "tfidf_matrix_test = tfidf_vectorizer.transform(df_test['clean_text'])\n",
        "\n",
        "# Predict on the test dataset\n",
        "test_predictions = model.predict(tfidf_matrix_test)\n",
        "test_predictions = np.clip(test_predictions, 1, 6)  # Clip predictions to range [1, 6]\n",
        "\n",
        "# Prepare the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'essay_id': df_test['essay_id'],\n",
        "    'score': test_predictions.round().astype(int)\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_filename = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/submission.csv'\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "print(f\"Submission file saved as {submission_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTtIzoYv2w8T"
      },
      "source": [
        "#**M2. XGB Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpyCDUdu2ypL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load training data\n",
        "train_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to 'full_text' column\n",
        "df_train['clean_text'] = df_train['full_text'].apply(preprocess_text)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = tfidf_vectorizer.fit_transform(df_train['clean_text'])\n",
        "y = df_train['score']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost model\n",
        "model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_valid_pred = model.predict(X_valid)\n",
        "y_valid_pred = np.clip(y_valid_pred, 1, 6)  # Clip predictions to range [1, 6]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
        "valid_kappa = cohen_kappa_score(y_valid.round().astype(int), y_valid_pred.round().astype(int), weights='quadratic')\n",
        "\n",
        "print(f\"Validation RMSE: {valid_rmse}\")\n",
        "print(f\"Validation Quadratic Weighted Kappa (Kappa): {valid_kappa}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXxvRPqI2wkX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Load the saved model and vectorizer\n",
        "model = joblib.load('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/essaymodel.h5')\n",
        "tfidf_vectorizer = joblib.load('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/tfidf_vectorizer.joblib')\n",
        "\n",
        "# Define function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing function to 'full_text' column\n",
        "df_test['clean_text'] = df_test['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Transform the cleaned text using the loaded TF-IDF vectorizer\n",
        "tfidf_matrix_test = tfidf_vectorizer.transform(df_test['clean_text'])\n",
        "\n",
        "# Predict on the test dataset\n",
        "test_predictions = model.predict(tfidf_matrix_test)\n",
        "test_predictions = np.clip(test_predictions, 1, 6)  # Clip predictions to range [1, 6]\n",
        "\n",
        "# Prepare the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'essay_id': df_test['essay_id'],\n",
        "    'score': test_predictions.round().astype(int)\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_filename = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/submission2.csv'\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "print(f\"Submission file saved as {submission_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ICEH2RvHRHz"
      },
      "source": [
        "#**M3.NN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0vvAUgEeljo"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchtext transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXvKbKEIfbMx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score, accuracy_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Assuming preprocess_text function and train.csv loading are defined as before\n",
        "# Replace with your actual implementations if different\n",
        "class EssayDataset(Dataset):\n",
        "    def __init__(self, texts, scores, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.scores = scores\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        score = float(self.scores[idx])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'score': torch.tensor(score, dtype=torch.float)\n",
        "        }\n",
        "class CNNTextClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_filters, filter_sizes, output_dim, dropout):\n",
        "        super(CNNTextClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(len(tokenizer), embedding_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = embedded.unsqueeze(1)  # add channel dimension (batch_size, channels, seq_len, embed_dim)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # convolution over embedding\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # max-pooling over time\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))  # concatenate pooled layers\n",
        "\n",
        "        return self.fc(cat)\n",
        "# Hyperparameters\n",
        "MAX_LEN = 512\n",
        "EMBEDDING_DIM = 100\n",
        "NUM_FILTERS = 100\n",
        "FILTER_SIZES = [3, 4, 5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Data preparation\n",
        "train_dataset = EssayDataset(df_train['clean_text'].tolist(), df_train['score'].tolist(), tokenizer, MAX_LEN)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Model initialization\n",
        "model = CNNTextClassifier(EMBEDDING_DIM, NUM_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        scores = batch['score']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs.squeeze(1), scores)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss / len(train_loader)}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        scores = batch['score']\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        predictions.extend(outputs.squeeze(1).tolist())\n",
        "        true_labels.extend(scores.tolist())\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "rmse = np.sqrt(mean_squared_error(true_labels, predictions))\n",
        "kappa = cohen_kappa_score(true_labels.round().astype(int), predictions.round().astype(int), weights='quadratic')\n",
        "accuracy = accuracy_score(true_labels.round().astype(int), predictions.round().astype(int))\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"Quadratic Weighted Kappa (Kappa): {kappa}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/cnn_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEkq-WVL0hLn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score, accuracy_score\n",
        "\n",
        "# Define EssayDataset class\n",
        "class EssayDataset(Dataset):\n",
        "    def __init__(self, texts, scores, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.scores = scores  # Dummy scores for test set\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "# Define CNNTextClassifier class\n",
        "class CNNTextClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_filters, filter_sizes, output_dim, dropout, vocab_size):\n",
        "        super(CNNTextClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = embedded.unsqueeze(1)  # add channel dimension (batch_size, channels, seq_len, embed_dim)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # convolution over embedding\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # max-pooling over time\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))  # concatenate pooled layers\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LEN = 512\n",
        "EMBEDDING_DIM = 100\n",
        "NUM_FILTERS = 100\n",
        "FILTER_SIZES = [3, 4, 5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
        "test_df['clean_text'] = test_df['full_text'].astype(str)  # Assuming 'full_text' is already cleaned if needed\n",
        "\n",
        "# Create test dataset and data loader\n",
        "test_dataset = EssayDataset(test_df['clean_text'].tolist(),\n",
        "                            [0]*len(test_df),  # Dummy scores since we predict these\n",
        "                            tokenizer,\n",
        "                            MAX_LEN)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "model = CNNTextClassifier(EMBEDDING_DIM, NUM_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, len(tokenizer))\n",
        "\n",
        "# Load trained model state dict\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/cnn_model.pt'))\n",
        "\n",
        "# Evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Predictions storage\n",
        "predictions = []\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        predictions.extend(outputs.squeeze(1).tolist())\n",
        "\n",
        "# Convert predictions to numpy array\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "# Prepare submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    'essay_id': test_df['essay_id'],\n",
        "    'score': predictions\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved successfully to submission.csv.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nttSNVux1J3f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score, accuracy_score\n",
        "\n",
        "# Define EssayDataset class\n",
        "class EssayDataset(Dataset):\n",
        "    def __init__(self, texts, scores, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.scores = scores  # Dummy scores for test set\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "# Define CNNTextClassifier class\n",
        "class CNNTextClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_filters, filter_sizes, output_dim, dropout, vocab_size):\n",
        "        super(CNNTextClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = embedded.unsqueeze(1)  # add channel dimension (batch_size, channels, seq_len, embed_dim)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # convolution over embedding\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # max-pooling over time\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))  # concatenate pooled layers\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LEN = 512\n",
        "EMBEDDING_DIM = 100\n",
        "NUM_FILTERS = 100\n",
        "FILTER_SIZES = [3, 4, 5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
        "test_df['clean_text'] = test_df['full_text'].astype(str)  # Assuming 'full_text' is already cleaned if needed\n",
        "\n",
        "# Create test dataset and data loader\n",
        "test_dataset = EssayDataset(test_df['clean_text'].tolist(),\n",
        "                            [0]*len(test_df),  # Dummy scores since we predict these\n",
        "                            tokenizer,\n",
        "                            MAX_LEN)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "model = CNNTextClassifier(EMBEDDING_DIM, NUM_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, len(tokenizer))\n",
        "\n",
        "# Load trained model state dict\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/cnn_model.pt'))\n",
        "\n",
        "# Evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Predictions storage\n",
        "predictions = []\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        predictions.extend(outputs.squeeze(1).tolist())\n",
        "\n",
        "# Convert predictions to numpy array\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "# Prepare submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    'essay_id': test_df['essay_id'],\n",
        "    'score': predictions\n",
        "})\n",
        "\n",
        "# Save submission file to specified path\n",
        "submission_df.to_csv('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/submission4.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved successfully to submission2.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLWd3bWRAqBe"
      },
      "source": [
        "#**M4 LGBM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czq4pyi0ArW0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import joblib\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load training data\n",
        "train_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to 'full_text' column\n",
        "df_train['clean_text'] = df_train['full_text'].apply(preprocess_text)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = tfidf_vectorizer.fit_transform(df_train['clean_text'])\n",
        "y = df_train['score']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize LightGBM model\n",
        "model = LGBMRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_valid_pred = model.predict(X_valid)\n",
        "y_valid_pred = np.clip(y_valid_pred, 1, 6)  # Clip predictions to range [1, 6]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
        "valid_kappa = cohen_kappa_score(y_valid.round().astype(int), y_valid_pred.round().astype(int), weights='quadratic')\n",
        "\n",
        "print(f\"Validation RMSE: {valid_rmse}\")\n",
        "print(f\"Validation Quadratic Weighted Kappa (Kappa): {valid_kappa}\")\n",
        "\n",
        "# Save the model\n",
        "model_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/AutomatedEssaymodel_LightGBM.h5'\n",
        "joblib.dump(model, model_save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii4BIn1mA1Tl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Load the saved model and vectorizer\n",
        "model = joblib.load('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/AutomatedEssaymodel_LightGBM.h5')\n",
        "tfidf_vectorizer = joblib.load('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/tfidf_vectorizer.joblib')\n",
        "\n",
        "# Define function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing function to 'full_text' column\n",
        "df_test['clean_text'] = df_test['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Transform the cleaned text using the loaded TF-IDF vectorizer\n",
        "tfidf_matrix_test = tfidf_vectorizer.transform(df_test['clean_text'])\n",
        "\n",
        "# Predict on the test dataset\n",
        "test_predictions = model.predict(tfidf_matrix_test)\n",
        "test_predictions = np.clip(test_predictions, 1, 6)  # Clip predictions to range [1, 6]\n",
        "\n",
        "# Prepare the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'essay_id': df_test['essay_id'],\n",
        "    'score': test_predictions.round().astype(int)\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_filename = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/submission_lightgbm.csv'\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "print(f\"Submission file saved as {submission_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt0dRUDbWyHJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOIyjbVYQpOt"
      },
      "source": [
        "#**M2. imporved XGBR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxAyGCV4S4AI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score\n",
        "import joblib\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load training data\n",
        "train_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to 'full_text' column\n",
        "df_train['clean_text'] = df_train['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Add additional features\n",
        "df_train['text_length'] = df_train['full_text'].apply(len)\n",
        "df_train['word_count'] = df_train['full_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_train['clean_text'])\n",
        "\n",
        "# Combine TF-IDF features with additional features\n",
        "X = np.hstack((X_tfidf.toarray(), df_train[['text_length', 'word_count']].values))\n",
        "y = df_train['score']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Setup the random search with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=50, scoring='neg_mean_squared_error', cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_xgb_model = XGBRegressor(**best_params)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_valid_pred = best_xgb_model.predict(X_valid)\n",
        "y_valid_pred = np.clip(y_valid_pred, 1, 6)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
        "valid_kappa = cohen_kappa_score(y_valid.round().astype(int), y_valid_pred.round().astype(int), weights='quadratic')\n",
        "\n",
        "print(f\"Validation RMSE: {valid_rmse}\")\n",
        "print(f\"Validation Quadratic Weighted Kappa: {valid_kappa}\")\n",
        "\n",
        "# Save the model to a file\n",
        "model_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/Model.h5'\n",
        "joblib.dump(best_xgb_model, model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYsOZmInFrT4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBRegressor\n",
        "import joblib\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Text preprocessing function (same as used for training data)\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to 'full_text' column in test data\n",
        "df_test['clean_text'] = df_test['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Load the trained TF-IDF vectorizer\n",
        "vectorizer_load_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/tfidf_vectorizer.pkl'\n",
        "tfidf_vectorizer = joblib.load(vectorizer_load_path)\n",
        "\n",
        "# TF-IDF Vectorization on test data\n",
        "X_tfidf_test = tfidf_vectorizer.transform(df_test['clean_text'])\n",
        "\n",
        "# Additional features for test data\n",
        "df_test['text_length'] = df_test['full_text'].apply(len)\n",
        "df_test['word_count'] = df_test['full_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Combine TF-IDF features with additional features\n",
        "X_test = np.hstack((X_tfidf_test.toarray(), df_test[['text_length', 'word_count']].values))\n",
        "\n",
        "# Load the trained XGBoost model\n",
        "model_load_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/Model.h5'\n",
        "best_xgb_model = joblib.load(model_load_path)\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = best_xgb_model.predict(X_test)\n",
        "y_test_pred = np.clip(y_test_pred, 1, 6)  # Clip predictions to the range [1, 6]\n",
        "\n",
        "# Prepare submission DataFrame\n",
        "submission_df = pd.DataFrame({'essay_id': df_test['essay_id'], 'predicted_score': y_test_pred})\n",
        "\n",
        "# Save submission to CSV file\n",
        "submission_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/submissions.csv'\n",
        "submission_df.to_csv(submission_save_path, index=False)\n",
        "print(f\"Submission saved to {submission_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSn46kc8K5hi"
      },
      "source": [
        "**M5. LSTM model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUKIO_2kK8Jf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Concatenate, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import joblib\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load training data\n",
        "train_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to 'full_text' column\n",
        "df_train['clean_text'] = df_train['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Tokenization and padding\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(df_train['clean_text'].values)\n",
        "X = tokenizer.texts_to_sequences(df_train['clean_text'].values)\n",
        "X = pad_sequences(X, maxlen=300)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/tokenizer.pkl'\n",
        "joblib.dump(tokenizer, tokenizer_save_path)\n",
        "\n",
        "# Additional features\n",
        "df_train['text_length'] = df_train['full_text'].apply(len)\n",
        "df_train['word_count'] = df_train['full_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Labels\n",
        "y = df_train['score'].values\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train_text, X_valid_text, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_additional, X_valid_additional = train_test_split(df_train[['text_length', 'word_count']].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Building the LSTM model with additional features\n",
        "embedding_dim = 100\n",
        "\n",
        "# Text input branch\n",
        "text_input = Input(shape=(300,), name='text_input')\n",
        "x = Embedding(input_dim=5000, output_dim=embedding_dim, input_length=300)(text_input)\n",
        "x = SpatialDropout1D(0.2)(x)\n",
        "x = Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "\n",
        "# Additional features input branch\n",
        "additional_input = Input(shape=(2,), name='additional_input')\n",
        "y = Dense(10, activation='relu')(additional_input)\n",
        "\n",
        "# Concatenate the two branches\n",
        "combined = Concatenate()([x, y])\n",
        "combined = Dense(50, activation='relu')(combined)\n",
        "combined = Dropout(0.2)(combined)\n",
        "output = Dense(1, activation='linear')(combined)\n",
        "\n",
        "model = Model(inputs=[text_input, additional_input], outputs=output)\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
        "print(model.summary())\n",
        "\n",
        "# Training the model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "history = model.fit([X_train_text, X_train_additional], y_train, epochs=20, batch_size=64, validation_data=([X_valid_text, X_valid_additional], y_valid), callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_valid_pred = model.predict([X_valid_text, X_valid_additional])\n",
        "y_valid_pred = np.clip(y_valid_pred, 1, 6).flatten()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
        "valid_kappa = cohen_kappa_score(y_valid.round().astype(int), y_valid_pred.round().astype(int), weights='quadratic')\n",
        "\n",
        "print(f\"Validation RMSE: {valid_rmse}\")\n",
        "print(f\"Validation Quadratic Weighted Kappa: {valid_kappa}\")\n",
        "\n",
        "# Save the model to a file\n",
        "model_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/LSTM_Model.h5'\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXgkYY6hSoZK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function for text preprocessing (same as used in training)\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Apply preprocessing to 'full_text' column in test data\n",
        "df_test['clean_text'] = df_test['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer_load_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/tokenizer.pkl'\n",
        "tokenizer = joblib.load(tokenizer_load_path)\n",
        "\n",
        "# Tokenization and padding for test data\n",
        "X_test = tokenizer.texts_to_sequences(df_test['clean_text'].values)\n",
        "X_test = pad_sequences(X_test, maxlen=300)\n",
        "\n",
        "# Load the trained LSTM model\n",
        "model_load_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/LSTM_Model.h5'\n",
        "model = load_model(model_load_path)\n",
        "\n",
        "# Additional features for test data\n",
        "df_test['text_length'] = df_test['full_text'].apply(len)\n",
        "df_test['word_count'] = df_test['full_text'].apply(lambda x: len(x.split()))\n",
        "X_test_additional = df_test[['text_length', 'word_count']].values\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = model.predict([X_test, X_test_additional])\n",
        "y_test_pred = np.clip(y_test_pred, 1, 6).flatten()\n",
        "y_test_pred = np.round(y_test_pred).astype(int)  # Round predictions and convert to integers\n",
        "\n",
        "# Prepare submission DataFrame\n",
        "submission_df = pd.DataFrame({'essay_id': df_test['essay_id'], 'predicted_score': y_test_pred})\n",
        "\n",
        "# Save submission to CSV file\n",
        "submission_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/submission100.csv'\n",
        "submission_df.to_csv(submission_save_path, index=False)\n",
        "print(f\"Submission saved to {submission_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPsWeLsrliR4"
      },
      "outputs": [],
      "source": [
        "!pip install pyspellchecker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDPEuS_Nl7Ic"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBRegressor\n",
        "import joblib\n",
        "from spellchecker import SpellChecker  # Ensure you have installed 'pyspellchecker' package\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Spell checker setup\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Text preprocessing function with spell checking\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Spell checking\n",
        "    tokens = word_tokenize(text)\n",
        "    corrected_tokens = [spell.correction(word) for word in tokens]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in corrected_tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Apply preprocessing to 'full_text' column in test data\n",
        "df_test['clean_text'] = df_test['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Load TF-IDF vectorizer\n",
        "vectorizer_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/Tfidf_Vectorizer.joblib'\n",
        "tfidf_vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "# Transform test data using TF-IDF vectorizer\n",
        "X_tfidf_test = tfidf_vectorizer.transform(df_test['clean_text'])\n",
        "\n",
        "# Additional features for test data\n",
        "df_test['text_length'] = df_test['full_text'].apply(len)\n",
        "df_test['word_count'] = df_test['full_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Combine TF-IDF features and additional features\n",
        "X_test_features = np.hstack((X_tfidf_test.toarray(), df_test[['text_length', 'word_count']].values))\n",
        "\n",
        "# Load the trained XGBoost model\n",
        "model_load_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/XGBModel.h5'\n",
        "xgb_model = joblib.load(model_load_path)\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = xgb_model.predict(X_test_features)\n",
        "y_test_pred = np.clip(y_test_pred, 1, 6)  # Clip predictions to the range [1, 6]\n",
        "\n",
        "# Prepare submission DataFrame\n",
        "submission_df = pd.DataFrame({'essay_id': df_test['essay_id'], 'predicted_score': y_test_pred})\n",
        "\n",
        "# Save submission to CSV file\n",
        "submission_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/submissions_xgb.csv'\n",
        "submission_df.to_csv(submission_save_path, index=False)\n",
        "print(f\"Submission saved to {submission_save_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDMFFJVav1sr"
      },
      "source": [
        "#**M2 imporved XGBR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsU7aAoBydq6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score\n",
        "import joblib\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load training data\n",
        "train_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to 'full_text' column\n",
        "df_train['clean_text'] = df_train['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Add additional features (excluding avg_sentence_length and flesch_reading_ease)\n",
        "df_train['text_length'] = df_train['full_text'].apply(len)\n",
        "df_train['word_count'] = df_train['full_text'].apply(lambda x: len(x.split()))\n",
        "df_train['avg_word_length'] = df_train['full_text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
        "df_train['sentence_count'] = df_train['full_text'].apply(lambda x: len(re.split(r'[.!?]', x)))\n",
        "\n",
        "# TF-IDF Vectorization with bi-grams and reduced max_features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_train['clean_text'])\n",
        "\n",
        "# Save the TF-IDF vectorizer\n",
        "vectorizer_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/xgbrtfidf_vectorizer.joblib'\n",
        "joblib.dump(tfidf_vectorizer, vectorizer_save_path)\n",
        "\n",
        "# Combine TF-IDF features with selected additional features\n",
        "X = np.hstack((X_tfidf.toarray(), df_train[['text_length', 'word_count', 'avg_word_length', 'sentence_count']].values))\n",
        "y = df_train['score']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Setup the random search with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=50, scoring='neg_mean_squared_error', cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_xgb_model = XGBRegressor(**best_params)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_valid_pred = best_xgb_model.predict(X_valid)\n",
        "y_valid_pred = np.clip(y_valid_pred, 1, 6)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
        "valid_kappa = cohen_kappa_score(y_valid.round().astype(int), y_valid_pred.round().astype(int), weights='quadratic')\n",
        "\n",
        "print(f\"Validation RMSE: {valid_rmse}\")\n",
        "print(f\"Validation Quadratic Weighted Kappa: {valid_kappa}\")\n",
        "\n",
        "# Save the model to a file\n",
        "model_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/XGBR-Model.h5'\n",
        "joblib.dump(best_xgb_model, model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs-HxpVsa2ig"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "from lightgbm import LGBMRegressor  # Import LightGBM\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to 'full_text' column in test data\n",
        "df_test['clean_text'] = df_test['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Load TF-IDF vectorizer\n",
        "vectorizer_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/xgbrtfidf_vectorizer.joblib'\n",
        "tfidf_vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "# Transform test data using TF-IDF vectorizer\n",
        "X_tfidf_test = tfidf_vectorizer.transform(df_test['clean_text'])\n",
        "\n",
        "# Additional features for test data\n",
        "df_test['text_length'] = df_test['full_text'].apply(len)\n",
        "df_test['word_count'] = df_test['full_text'].apply(lambda x: len(x.split()))\n",
        "df_test['avg_word_length'] = df_test['full_text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
        "df_test['sentence_count'] = df_test['full_text'].apply(lambda x: len(re.split(r'[.!?]', x)))\n",
        "\n",
        "# Combine TF-IDF features with selected additional features\n",
        "X_test_features = np.hstack((X_tfidf_test.toarray(), df_test[['text_length', 'word_count', 'avg_word_length', 'sentence_count']].values))\n",
        "\n",
        "# Load the trained LightGBM model\n",
        "model_load_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/XGBR-Model.h5'\n",
        "lgbm_model = joblib.load(model_load_path)\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = lgbm_model.predict(X_test_features)\n",
        "y_test_pred = np.round(y_test_pred).astype(int)  # Round predictions to nearest integer\n",
        "y_test_pred = np.clip(y_test_pred, 1, 6)  # Clip predictions to the range [1, 6]\n",
        "\n",
        "# Prepare submission DataFrame\n",
        "submission_df = pd.DataFrame({'essay_id': df_test['essay_id'], 'predicted_score': y_test_pred})\n",
        "\n",
        "# Save submission to CSV file\n",
        "submission_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/submission_xgbrfeturd.csv'\n",
        "submission_df.to_csv(submission_save_path, index=False)\n",
        "print(f\"Submission saved to {submission_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0IjPJM8ZZWi"
      },
      "source": [
        "**xgbr with some new faetures**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOdXaykzZYwR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Text preprocessing function (same as used for training data)\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to 'full_text' column in test data\n",
        "df_test['clean_text'] = df_test['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Load the trained TF-IDF vectorizer\n",
        "vectorizer_load_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/xgbrlineartfidf_vectorizer.joblib'\n",
        "tfidf_vectorizer = joblib.load(vectorizer_load_path)\n",
        "\n",
        "# TF-IDF Vectorization on test data\n",
        "X_tfidf_test = tfidf_vectorizer.transform(df_test['clean_text'])\n",
        "\n",
        "# Additional features for test data\n",
        "df_test['text_length'] = df_test['full_text'].apply(len)\n",
        "df_test['word_count'] = df_test['full_text'].apply(lambda x: len(x.split()))\n",
        "df_test['avg_word_length'] = df_test['full_text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
        "df_test['sentiment'] = df_test['full_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "# Combine TF-IDF features with additional features\n",
        "X_test = np.hstack((X_tfidf_test.toarray(), df_test[['text_length', 'word_count', 'avg_word_length', 'sentiment']].values))\n",
        "\n",
        "# Load the trained XGBoost model\n",
        "model_load_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/xgbrlinearModel.h5'\n",
        "best_ensemble_model = joblib.load(model_load_path)\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = best_ensemble_model.predict(X_test)\n",
        "y_test_pred_rounded = np.round(y_test_pred).astype(int)  # Round predictions to nearest integer\n",
        "y_test_pred_clipped = np.clip(y_test_pred_rounded, 1, 6)  # Clip predictions to the range [1, 6]\n",
        "\n",
        "# Prepare submission DataFrame\n",
        "submission_df = pd.DataFrame({'essay_id': df_test['essay_id'], 'predicted_score': y_test_pred_clipped})\n",
        "\n",
        "# Save submission to CSV file\n",
        "submission_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/xgbrlinearsubmissions.csv'\n",
        "submission_df.to_csv(submission_save_path, index=False)\n",
        "print(f\"Submission saved to {submission_save_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFDFkXdLz_dg"
      },
      "source": [
        "#**M2 imporved xgbr**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6t1JkwxfXhu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score\n",
        "import joblib\n",
        "from scipy.stats import kurtosis\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Function to remove HTML tags\n",
        "def removeHTML(x):\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', x)\n",
        "\n",
        "# Function for comprehensive data preprocessing\n",
        "def dataPreprocessing(x):\n",
        "    x = x.lower()\n",
        "    x = removeHTML(x)\n",
        "    x = re.sub(\"@\\w+\", '', x)\n",
        "    x = re.sub(\"http\\w+\", '', x)\n",
        "    x = re.sub(r\"\\s+\", \" \", x)\n",
        "    x = re.sub(r\"\\.+\", \".\", x)\n",
        "    x = re.sub(r\"\\,+\", \",\", x)\n",
        "    x = x.strip()\n",
        "    return x\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Function to count spelling errors\n",
        "def count_spelling_errors(text):\n",
        "    # Implement your own logic or use a library like spaCy to count spelling errors\n",
        "    return 0  # Placeholder\n",
        "\n",
        "# Function to preprocess words\n",
        "def Word_Preprocess(tmp):\n",
        "    tmp['clean_text'] = tmp['full_text'].apply(dataPreprocessing)\n",
        "    words = tmp[['essay_id', 'clean_text']].copy()\n",
        "    words = words.assign(word=words['clean_text'].str.split()).explode('word').reset_index(drop=True)\n",
        "    words['word_len'] = words['word'].apply(len)\n",
        "    words = words[words['word_len'] > 0]  # Remove words with length 0\n",
        "    return words\n",
        "\n",
        "# Function for word feature engineering\n",
        "def Word_Eng(words):\n",
        "    word_aggs = {\n",
        "        'word_len': ['max', 'mean', 'std', lambda x: x.quantile(0.25), lambda x: x.quantile(0.50), lambda x: x.quantile(0.75)]\n",
        "    }\n",
        "    essay_features = words.groupby('essay_id').agg(word_aggs)\n",
        "    essay_features.columns = ['_'.join(col).strip() for col in essay_features.columns.values]\n",
        "    return essay_features.reset_index()\n",
        "\n",
        "# Function to preprocess sentences\n",
        "def Sentence_Preprocess(tmp):\n",
        "    tmp['clean_text'] = tmp['full_text'].apply(dataPreprocessing)\n",
        "    sentences = tmp[['essay_id', 'clean_text']].copy()\n",
        "    sentences = sentences.assign(sentence=sentences['clean_text'].str.split('.')).explode('sentence').reset_index(drop=True)\n",
        "    sentences['sentence_len'] = sentences['sentence'].apply(len)\n",
        "    sentences['sentence_word_cnt'] = sentences['sentence'].apply(lambda x: len(x.split()))\n",
        "    sentences = sentences[sentences['sentence_len'] >= 15]  # Filter sentences with length >= 15\n",
        "    return sentences\n",
        "\n",
        "# Function for sentence feature engineering\n",
        "def Sentence_Eng(sentences):\n",
        "    sentence_aggs = {\n",
        "        'sentence_len': ['count', 'max', 'mean', 'min', 'sum', 'first', 'last', kurtosis, lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)],\n",
        "        'sentence_word_cnt': ['max', 'mean', 'min', 'sum', 'first', 'last', kurtosis, lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)]\n",
        "    }\n",
        "    essay_features = sentences.groupby('essay_id').agg(sentence_aggs)\n",
        "    essay_features.columns = ['_'.join(col).strip() for col in essay_features.columns.values]\n",
        "    return essay_features.reset_index()\n",
        "\n",
        "# Function to preprocess paragraphs\n",
        "def Paragraph_Preprocess(tmp):\n",
        "    tmp = tmp.explode('paragraph')\n",
        "    tmp['clean_paragraph'] = tmp['paragraph'].apply(dataPreprocessing)\n",
        "    tmp['paragraph_no_punctuation'] = tmp['clean_paragraph'].apply(remove_punctuation)\n",
        "    tmp['paragraph_error_num'] = tmp['paragraph_no_punctuation'].apply(count_spelling_errors)\n",
        "    tmp['paragraph_len'] = tmp['paragraph'].apply(len)\n",
        "    tmp['paragraph_sentence_cnt'] = tmp['paragraph'].apply(lambda x: len(re.split(r'\\.|!|\\?', x)))\n",
        "    tmp['paragraph_word_cnt'] = tmp['paragraph'].apply(lambda x: len(x.split()))\n",
        "    return tmp\n",
        "\n",
        "# Function for paragraph feature engineering\n",
        "def Paragraph_Eng(paragraphs):\n",
        "    paragraph_aggs = {\n",
        "        'paragraph_len': ['max', 'mean', 'std', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)],\n",
        "        'paragraph_sentence_cnt': ['max', 'mean', 'std', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)],\n",
        "        'paragraph_word_cnt': ['max', 'mean', 'std', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)],\n",
        "        'paragraph_error_num': ['max', 'mean', 'std', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)]\n",
        "    }\n",
        "    essay_features = paragraphs.groupby('essay_id').agg(paragraph_aggs)\n",
        "    essay_features.columns = ['_'.join(col).strip() for col in essay_features.columns.values]\n",
        "    return essay_features.reset_index()\n",
        "\n",
        "# Load training data\n",
        "train_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Apply preprocessing to 'full_text' column\n",
        "df_train['clean_text'] = df_train['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Additional textual features\n",
        "df_train['num_chars'] = df_train['full_text'].apply(len)\n",
        "df_train['num_words'] = df_train['full_text'].apply(lambda x: len(x.split()))\n",
        "df_train['num_sentences'] = df_train['full_text'].apply(lambda x: len(sent_tokenize(x)))\n",
        "df_train['AvgWordLength'] = df_train['num_chars'] / df_train['num_words']\n",
        "\n",
        "# Word features\n",
        "words_df = Word_Preprocess(df_train)\n",
        "word_features = Word_Eng(words_df)\n",
        "df_train = df_train.merge(word_features, on='essay_id', how='left')\n",
        "\n",
        "# Sentence features\n",
        "sentences_df = Sentence_Preprocess(df_train)\n",
        "sentence_features = Sentence_Eng(sentences_df)\n",
        "df_train = df_train.merge(sentence_features, on='essay_id', how='left')\n",
        "\n",
        "# Paragraph features\n",
        "df_train['paragraph'] = df_train['full_text'].str.split('\\n')\n",
        "paragraphs_df = Paragraph_Preprocess(df_train)\n",
        "paragraph_features = Paragraph_Eng(paragraphs_df)\n",
        "df_train = df_train.merge(paragraph_features, on='essay_id', how='left')\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_train['clean_text'])\n",
        "\n",
        "# Combine TF-IDF features with additional features\n",
        "X = np.hstack((X_tfidf.toarray(), df_train[['num_chars', 'num_words', 'num_sentences', 'AvgWordLength']].values))\n",
        "y = df_train['score']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Setup the random search with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=50, scoring='neg_mean_squared_error', cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_xgb_model = XGBRegressor(**best_params)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_valid_pred = best_xgb_model.predict(X_valid)\n",
        "y_valid_pred = np.clip(y_valid_pred, 1, 6)  # Assuming scores range from 1 to 6\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
        "valid_kappa = cohen_kappa_score(y_valid.round().astype(int), y_valid_pred.round().astype(int), weights='quadratic')\n",
        "\n",
        "print(f\"Validation RMSE: {valid_rmse}\")\n",
        "print(f\"Validation Quadratic Weighted Kappa: {valid_kappa}\")\n",
        "\n",
        "# Save the model to a file\n",
        "model_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/EssayModel.h5'\n",
        "joblib.dump(best_xgb_model, model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "# Save the TF-IDF vectorizer\n",
        "vectorizer_save_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/Essaytfidf_vectorizer.joblib'\n",
        "joblib.dump(tfidf_vectorizer, vectorizer_save_path)\n",
        "print(f\"TF-IDF Vectorizer saved to {vectorizer_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0HHb5xtWFCA"
      },
      "source": [
        "# **M6.deberta**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-Q1XoV8WGFb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import AutoTokenizer, DebertaForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "\n",
        "# Define path to your train.csv file\n",
        "train_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv'\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Adjust labels to start from 0 (assuming score ranges from 1 to 6)\n",
        "df_train['labels'] = df_train['score'] - 1\n",
        "\n",
        "# Initialize and train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=df_train['full_text'].apply(str.split).tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to embed text using Word2Vec model\n",
        "def embed_text(text):\n",
        "    embedded_text = []\n",
        "    for word in text.split():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedded_text.append(word2vec_model.wv[word])\n",
        "        else:\n",
        "            embedded_text.append(np.zeros(word2vec_model.vector_size))  # Handle out-of-vocabulary words\n",
        "    return np.mean(embedded_text, axis=0)  # Average embeddings of words in the text\n",
        "\n",
        "# Tokenizer for the input text\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['full_text'], padding=True, truncation=True)\n",
        "\n",
        "# Prepare datasets\n",
        "ds_train = Dataset.from_pandas(df_train)\n",
        "\n",
        "# Embedding datasets with Word2Vec embeddings\n",
        "ds_train = ds_train.map(lambda x: {'embeddings': embed_text(x['full_text'])}, batched=True)\n",
        "\n",
        "# Define training and validation sets\n",
        "train_dataset, val_dataset = train_test_split(ds_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define training arguments\n",
        "train_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type='linear',\n",
        "    gradient_accumulation_steps=4,\n",
        "    report_to='none',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Define model\n",
        "model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=6)\n",
        "\n",
        "# Define compute metrics function (for evaluation)\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n",
        "    return {'cohen_kappa': kappa}\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=train_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/deberta-v3-trained-model')\n",
        "\n",
        "# Optionally, save tokenizer\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/deberta-v3-tokenizer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOYS3YSg7-z"
      },
      "source": [
        "#**M5. BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbfI3daghAzX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score, accuracy_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Assuming preprocess_text function is defined\n",
        "def preprocess_text(text):\n",
        "    # Add your text preprocessing steps here\n",
        "    return text\n",
        "\n",
        "class EssayDataset(Dataset):\n",
        "    def __init__(self, texts, scores, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.scores = scores\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        score = float(self.scores[idx])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'score': torch.tensor(score, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class CNNTextClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_filters, filter_sizes, output_dim, dropout):\n",
        "        super(CNNTextClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(len(tokenizer), embedding_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = embedded.unsqueeze(1)  # add channel dimension (batch_size, channels, seq_len, embed_dim)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # convolution over embedding\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # max-pooling over time\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))  # concatenate pooled layers\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LEN = 512\n",
        "OUTPUT_DIM = 1\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
        "df['clean_text'] = df['full_text'].apply(preprocess_text)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data preparation\n",
        "train_dataset = EssayDataset(df_train['clean_text'].tolist(), df_train['score'].tolist(), tokenizer, MAX_LEN)\n",
        "val_dataset = EssayDataset(df_val['clean_text'].tolist(), df_val['score'].tolist(), tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Hyperparameter search space\n",
        "param_grid = {\n",
        "    'embedding_dim': [50, 100, 200],\n",
        "    'num_filters': [50, 100, 200],\n",
        "    'filter_sizes': [[2, 3, 4], [3, 4, 5], [4, 5, 6]],\n",
        "    'dropout': [0.3, 0.5, 0.7],\n",
        "    'learning_rate': [0.001, 0.0001, 0.00001]\n",
        "}\n",
        "\n",
        "best_rmse = float('inf')\n",
        "best_params = {}\n",
        "\n",
        "for embedding_dim in param_grid['embedding_dim']:\n",
        "    for num_filters in param_grid['num_filters']:\n",
        "        for filter_sizes in param_grid['filter_sizes']:\n",
        "            for dropout in param_grid['dropout']:\n",
        "                for learning_rate in param_grid['learning_rate']:\n",
        "                    # Model initialization\n",
        "                    model = CNNTextClassifier(embedding_dim, num_filters, filter_sizes, OUTPUT_DIM, dropout)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "                    criterion = nn.MSELoss()\n",
        "\n",
        "                    # Training loop\n",
        "                    model.train()\n",
        "                    for epoch in range(NUM_EPOCHS):\n",
        "                        epoch_loss = 0\n",
        "                        for batch in train_loader:\n",
        "                            input_ids = batch['input_ids']\n",
        "                            attention_mask = batch['attention_mask']\n",
        "                            scores = batch['score']\n",
        "\n",
        "                            optimizer.zero_grad()\n",
        "                            outputs = model(input_ids, attention_mask)\n",
        "                            loss = criterion(outputs.squeeze(1), scores)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            epoch_loss += loss.item()\n",
        "\n",
        "                    # Evaluation\n",
        "                    model.eval()\n",
        "                    predictions = []\n",
        "                    true_labels = []\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        for batch in val_loader:\n",
        "                            input_ids = batch['input_ids']\n",
        "                            attention_mask = batch['attention_mask']\n",
        "                            scores = batch['score']\n",
        "\n",
        "                            outputs = model(input_ids, attention_mask)\n",
        "                            predictions.extend(outputs.squeeze(1).tolist())\n",
        "                            true_labels.extend(scores.tolist())\n",
        "\n",
        "                    predictions = np.array(predictions)\n",
        "                    true_labels = np.array(true_labels)\n",
        "\n",
        "                    # Compute evaluation metrics\n",
        "                    rmse = np.sqrt(mean_squared_error(true_labels, predictions))\n",
        "\n",
        "                    if rmse < best_rmse:\n",
        "                        best_rmse = rmse\n",
        "                        best_params = {\n",
        "                            'embedding_dim': embedding_dim,\n",
        "                            'num_filters': num_filters,\n",
        "                            'filter_sizes': filter_sizes,\n",
        "                            'dropout': dropout,\n",
        "                            'learning_rate': learning_rate\n",
        "                        }\n",
        "\n",
        "print(f\"Best RMSE: {best_rmse}\")\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Use the best hyperparameters to train the final model\n",
        "model = CNNTextClassifier(\n",
        "    best_params['embedding_dim'],\n",
        "    best_params['num_filters'],\n",
        "    best_params['filter_sizes'],\n",
        "    OUTPUT_DIM,\n",
        "    best_params['dropout']\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop with the best hyperparameters\n",
        "model.train()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        scores = batch['score']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs.squeeze(1), scores)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss / len(train_loader)}')\n",
        "\n",
        "# Evaluation with the best hyperparameters\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        scores = batch['score']\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        predictions.extend(outputs.squeeze(1).tolist())\n",
        "        true_labels.extend(scores.tolist())\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "rmse = np.sqrt(mean_squared_error(true_labels, predictions))\n",
        "kappa = cohen_kappa_score(true_labels.round().astype(int), predictions.round().astype(int), weights='quadratic')\n",
        "accuracy = accuracy_score(true_labels.round().astype(int), predictions.round().astype(int))\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"Quadratic Weighted Kappa (Kappa): {kappa}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/nn_modelessay scoring.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlt8KuEO0zZa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, cohen_kappa_score, accuracy_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Load training data\n",
        "train_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/train.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Train-test split\n",
        "train_texts, val_texts, train_scores, val_scores = train_test_split(df_train['full_text'].tolist(), df_train['score'].tolist(), test_size=0.2, random_state=42)\n",
        "\n",
        "class EssayDataset(Dataset):\n",
        "    def __init__(self, texts, scores, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.scores = scores\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        score = float(self.scores[idx])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'score': torch.tensor(score, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class TextToImageCNN(nn.Module):\n",
        "    def __init__(self, bert_model, embedding_dim, num_filters, filter_sizes, output_dim, dropout):\n",
        "        super(TextToImageCNN, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.no_grad():\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Convert BERT embeddings to a 2D image-like representation\n",
        "        embedded = bert_output.last_hidden_state.unsqueeze(1)  # (batch_size, 1, seq_len, embedding_dim)\n",
        "\n",
        "        conv_results = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv_layers]\n",
        "        pooled_results = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conv_results]\n",
        "        concatenated = self.dropout(torch.cat(pooled_results, dim=1))\n",
        "\n",
        "        return self.fc(concatenated)\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LEN = 512\n",
        "EMBEDDING_DIM = 768  # BERT embedding size\n",
        "NUM_FILTERS = 256\n",
        "FILTER_SIZES = [3, 5, 7]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.4\n",
        "LEARNING_RATE = 0.0001\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 50\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "\n",
        "# Tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Data preparation\n",
        "train_dataset = EssayDataset(train_texts, train_scores, tokenizer, MAX_LEN)\n",
        "val_dataset = EssayDataset(val_texts, val_scores, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Model initialization\n",
        "model = TextToImageCNN(bert_model, EMBEDDING_DIM, NUM_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Mixed precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(tqdm.tqdm(train_loader, desc=\"Training\")):\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "        scores = batch['score'].to(device, non_blocking=True)\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs.squeeze(1), scores) / GRADIENT_ACCUMULATION_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss}, Time: {time.time() - start_time:.2f}s')\n",
        "\n",
        "    # Evaluation after each epoch\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "            scores = batch['score'].to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            predictions.extend(outputs.squeeze(1).tolist())\n",
        "            true_labels.extend(scores.tolist())\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    rmse = np.sqrt(mean_squared_error(true_labels, predictions))\n",
        "    kappa = cohen_kappa_score(true_labels.round().astype(int), predictions.round().astype(int), weights='quadratic')\n",
        "    accuracy = accuracy_score(true_labels.round().astype(int), predictions.round().astype(int))\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Evaluation Metrics:\")\n",
        "    print(f\"RMSE: {rmse}\")\n",
        "    print(f\"Quadratic Weighted Kappa (Kappa): {kappa}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/cnnlstms_models.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR3xvGx6ZW6V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm\n",
        "\n",
        "# Define the EssayDataset class\n",
        "class EssayDataset(Dataset):\n",
        "    def __init__(self, texts, scores, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.scores = scores\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        score = float(self.scores[idx])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'score': torch.tensor(score, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Define the TextToImageCNN model class\n",
        "class TextToImageCNN(nn.Module):\n",
        "    def __init__(self, bert_model, embedding_dim, num_filters, filter_sizes, output_dim, dropout):\n",
        "        super(TextToImageCNN, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.no_grad():\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Convert BERT embeddings to a 2D image-like representation\n",
        "        embedded = bert_output.last_hidden_state.unsqueeze(1)  # (batch_size, 1, seq_len, embedding_dim)\n",
        "\n",
        "        conv_results = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv_layers]\n",
        "        pooled_results = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conv_results]\n",
        "        concatenated = self.dropout(torch.cat(pooled_results, dim=1))\n",
        "\n",
        "        return self.fc(concatenated)\n",
        "\n",
        "# Parameters\n",
        "MAX_LEN = 512\n",
        "EMBEDDING_DIM = 768  # BERT embedding size\n",
        "NUM_FILTERS = 256\n",
        "FILTER_SIZES = [3, 5, 7]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.4\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Load the tokenizer and BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Initialize the model\n",
        "model = TextToImageCNN(bert_model, EMBEDDING_DIM, NUM_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the saved model weights with map_location\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/cnnlstms_models.pt', map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
        "df_test = pd.read_csv(test_path)\n",
        "test_texts = df_test['full_text'].tolist()\n",
        "test_ids = df_test['essay_id'].tolist()\n",
        "\n",
        "# Prepare test dataset and loader\n",
        "test_dataset = EssayDataset(test_texts, [0]*len(test_texts), tokenizer, MAX_LEN)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Predictions\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm.tqdm(test_loader, desc=\"Predicting\"):\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        predictions.extend(outputs.squeeze(1).tolist())\n",
        "\n",
        "# Convert predictions to the required integer range (1-6)\n",
        "predictions = np.round(predictions).astype(int)\n",
        "predictions = np.clip(predictions, 1, 6)\n",
        "\n",
        "# Create submission dataframe\n",
        "submission = pd.DataFrame({'essay_id': test_ids, 'score': predictions})\n",
        "\n",
        "# Save submission file\n",
        "submission_path = '/content/drive/MyDrive/kaggleC/learning-agency-lab-automated-essay-scoring-2/submissionlstmcnn.csv'\n",
        "submission.to_csv(submission_path, index=False)\n",
        "\n",
        "print(\"Submission file saved to:\", submission_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "data = {\n",
        "    \"Index\": [1, 2, 3, 4, 5],\n",
        "    \"Model\": [\"Linear Regression\",\"XGBoost Regressor\",\n",
        "              \"BERT\", \"LSTM\", \"LightGBM (LGBM)\"],\n",
        "    \"Cohen's Kappa\": [0.654, 0.710,0.7806, 0.771, 0.721]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print table\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Define colors (Red for NN and BERT)\n",
        "colors = [\"#1E3A5F\", \"#104E8B\", \"#0B3D91\", \"#166534\", \"#004D00\", \"#002400\"]\n",
        "# Light blue, sky blue, steel blue, light green, lime green, green\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(df[\"Model\"], df[\"Cohen's Kappa\"], color=colors)\n",
        "plt.xlabel(\"Cohen's Kappa Score\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wDUZcXq5BSwd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}